# Release - Tier 4: Complete test suite for production releases
# Target: <20 minutes execution time
# Scope: 1,449 tests including all migrations and external dependencies

name: Tier 4 - Release Validation

on:
  push:
    tags:
      - 'v*'
  release:
    types: [published]
  schedule:
    # Run full test suite nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      deploy_environment:
        description: 'Target deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

jobs:
  full-backend-test-suite:
    name: Complete Backend Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12"]
        test-group: [
          "api-core",
          "scrapers-batch1", 
          "scrapers-batch2",
          "services-utils",
          "integration-migrations"
        ]
        
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: rescue_dogs_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pytest_cache
        key: release-${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          release-${{ runner.os }}-python-${{ matrix.python-version }}-
          release-${{ runner.os }}-python-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Setup complete test database
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/rescue_dogs_test
      run: |
        # Run all migrations to ensure production-like environment
        python -c "
        import psycopg2
        conn = psycopg2.connect('postgresql://test_user:test_password@localhost:5432/rescue_dogs_test')
        with open('database/schema.sql', 'r') as f:
            with conn.cursor() as cur:
                cur.execute(f.read())
        conn.commit()
        conn.close()
        "
        
        # Apply all database migrations
        for migration in database/migrations/*.sql; do
          if [ -f "$migration" ]; then
            psql postgresql://test_user:test_password@localhost:5432/rescue_dogs_test -f "$migration"
          fi
        done

    - name: Run Test Group - ${{ matrix.test-group }}
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/rescue_dogs_test
        REDIS_URL: redis://localhost:6379/0
        PYTHONPATH: .
        CI: true
        TEST_ENVIRONMENT: release
        R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
        R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        R2_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}
        R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
      run: |
        case "${{ matrix.test-group }}" in
          "api-core")
            pytest tests/api/ -v --tb=short --durations=10 --cov=api
            ;;
          "scrapers-batch1")
            pytest tests/scrapers/ -k "not (misis or theunderdog or tierschutz)" -v --tb=short --durations=10 --cov=scrapers
            ;;
          "scrapers-batch2") 
            pytest tests/scrapers/ -k "(misis or theunderdog or tierschutz)" -v --tb=short --durations=10 --cov=scrapers
            ;;
          "services-utils")
            pytest tests/services/ tests/utils/ -v --tb=short --durations=10 --cov=services --cov=utils
            ;;
          "integration-migrations")
            pytest tests/integration/ tests/database/ tests/railway/ -v --tb=short --durations=10
            ;;
        esac

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.test-group }}
        path: |
          .coverage*
          test-results*.xml

  complete-frontend-validation:
    name: Complete Frontend Validation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    defaults:
      run:
        working-directory: ./frontend

    strategy:
      fail-fast: false
      matrix:
        node-version: ['18', '20']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install dependencies
      run: npm ci

    - name: Run complete test suite
      run: |
        npm test -- --coverage --watchAll=false --maxWorkers=2 --verbose

    - name: Full E2E test suite
      run: |
        npm run test:e2e:full

    - name: Performance and accessibility tests
      run: |
        npm run test:lighthouse || true

    - name: Build and deploy validation
      run: |
        npm run build
        npm run start:test & 
        sleep 10
        curl -f http://localhost:3000 || exit 1

  deployment-readiness:
    name: Deployment Readiness
    runs-on: ubuntu-latest
    needs: [full-backend-test-suite, complete-frontend-validation]
    if: always()
    timeout-minutes: 5

    steps:
    - name: Collect all test results
      run: |
        backend_result="${{ needs.full-backend-test-suite.result }}"
        frontend_result="${{ needs.complete-frontend-validation.result }}"
        
        echo "=== TIER 4 RELEASE VALIDATION RESULTS ==="
        echo "Backend Test Suite: $backend_result"
        echo "Frontend Validation: $frontend_result"
        
        if [[ "$backend_result" != "success" ]] || [[ "$frontend_result" != "success" ]]; then
          echo "❌ Release validation failed"
          echo "🚫 Deployment blocked"
          exit 1
        else
          echo "✅ All Tier 4 release validations passed"
          echo "📊 Complete test suite: 1,449+ tests executed"
          echo "🔒 Production-ready validation complete"
          echo "🚀 Ready for production deployment"
        fi

    - name: Trigger deployment
      if: success() && github.event_name == 'release'
      uses: actions/github-script@v7
      with:
        script: |
          const { owner, repo } = context.repo;
          
          await github.rest.actions.createWorkflowDispatch({
            owner,
            repo,
            workflow_id: 'deploy.yml',
            ref: context.ref,
            inputs: {
              environment: 'production',
              version: context.ref.replace('refs/tags/', '')
            }
          });
          
          console.log('🚀 Production deployment triggered');

  performance-benchmarking:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: [full-backend-test-suite]
    if: success()
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt -r requirements-dev.txt

    - name: Benchmark all tiers
      run: |
        source venv/bin/activate 2>/dev/null || echo "Using system Python"
        
        echo "=== TIERED TEST PERFORMANCE BENCHMARK ==="
        
        echo "Tier 1 - Developer Feedback:"
        time pytest -m "unit or fast" --maxfail=5 -x -q --tb=no | head -1
        
        echo "Tier 2 - CI Pipeline:"  
        time pytest -m "not slow and not browser and not external" --maxfail=3 -q --tb=no | head -1
        
        echo "Tier 3 - Pre-merge:"
        time pytest -m "not requires_migrations" --maxfail=1 -q --tb=no | head -1
        
        echo "Tier 4 - Full Release:"
        time pytest -q --tb=no | head -1
        
        echo "=== BENCHMARK COMPLETE ==="