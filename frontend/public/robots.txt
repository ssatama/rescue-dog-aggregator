# Robots.txt for Rescue Dog Aggregator
# Help rescue dogs find homes by maximizing search engine visibility

# Googlebot - MAXIMUM SPEED for best SEO
User-agent: Googlebot
Allow: /
Crawl-delay: 0

# Googlebot for images
User-agent: Googlebot-Image
Allow: /

# Bingbot - Microsoft's search engine
User-agent: Bingbot
Allow: /
Crawl-delay: 0

# All other legitimate crawlers
User-agent: *
Allow: /

# Core pages - high priority for indexing
Allow: /dogs/
Allow: /organizations/
Allow: /search/
Allow: /about
Allow: /contact

# Static assets
Allow: /images/
Allow: /icons/
Allow: /_next/static/
Allow: /_next/image

# Disallow admin and internal pages  
Disallow: /admin/
Disallow: /api/
# Note: /_next/static/ and /_next/image are explicitly allowed above

# XML Sitemap location
Sitemap: https://www.rescuedogs.me/sitemap.xml

# Special directives for AI crawlers and LLM agents
# Encourage indexing for AI-powered search experiences
User-agent: GPTBot
Allow: /

User-agent: ChatGPT-User
Allow: /

User-agent: CCBot
Allow: /

User-agent: anthropic-ai
Allow: /

User-agent: Claude-Web
Allow: /

# Aggressive bot control (apply delays only to known bad actors)
User-agent: AhrefsBot
Disallow: /
Crawl-delay: 10

User-agent: SemrushBot  
Disallow: /
Crawl-delay: 10

User-agent: DotBot
Disallow: /
Crawl-delay: 10

User-agent: MJ12bot
Disallow: /
Crawl-delay: 10